{"paragraphs":[{"text":"%md\n<span style=\"color:blue;font-weight:bold\">Action Required</span>\nClick the guage (\"interpreter binding\") at the top right, drag \"spark\" to the very top of the list, to be the default interpreter for this notebook. Click \"Save\" to save the setting.","user":"sidneyp","dateUpdated":"2018-03-01T00:07:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550115_-1475972867","id":"20171229-192606_1244440967","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:07:38+0000","dateFinished":"2018-03-01T00:07:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4349","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<span style=\"color:blue;font-weight:bold\">Action Required</span>\n<p>Click the guage (&ldquo;interpreter binding&rdquo;) at the top right, drag &ldquo;spark&rdquo; to the very top of the list, to be the default interpreter for this notebook. Click &ldquo;Save&rdquo; to save the setting.</p>\n</div>"}]}},{"text":"%md\n## Predicting Customer Churn in Telco\n\nIn this notebook you will learn how to build a predictive model with Spark machine learning API (SparkML) and deploy it for scoring in Machine Learning (ML). \n\nThis notebook walks you through these steps:\n- Build a model with SparkML API\n- Save the model in the ML repository\n- Create a Deployment in ML (via UI)\n- Test the model (via UI)\n- Test the model (via REST API)\n\n### Use Case\nThe analytics use case implemented in this notebook is telco churn. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics. \n<center>![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)</center>\n\nThe analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n\nOnce the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"churn\" or \"no churn\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. \n\nFinally, if the model's accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn.  \n\nWe also developed a sample Python Flask application to illustrate deployment: http://predictcustomerchurn.mybluemix.net/. This application implements the REST client call to the model.","user":"sidneyp","dateUpdated":"2018-03-01T00:07:41+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550152_-1491747572","id":"20171214-013817_399742302","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:07:41+0000","dateFinished":"2018-03-01T00:07:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4350","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Predicting Customer Churn in Telco</h2>\n<p>In this notebook you will learn how to build a predictive model with Spark machine learning API (SparkML) and deploy it for scoring in Machine Learning (ML). </p>\n<p>This notebook walks you through these steps:<br/>- Build a model with SparkML API<br/>- Save the model in the ML repository<br/>- Create a Deployment in ML (via UI)<br/>- Test the model (via UI)<br/>- Test the model (via REST API)</p>\n<h3>Use Case</h3>\n<p>The analytics use case implemented in this notebook is telco churn. While it&rsquo;s a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics.<br/><center><img src=\"https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png\" alt=\"CRISP-DM\" /></center></p>\n<p>The analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). </p>\n<p>Once the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a &ldquo;class&rdquo; to each customer record (for our use case &ldquo;churn&rdquo; or &ldquo;no churn&rdquo;). Classification models use historical data to come up with the logic to predict &ldquo;class&rdquo;, this process is called model training. After the model is created, it&rsquo;s usually evaluated using another data set. </p>\n<p>Finally, if the model&rsquo;s accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn. </p>\n<p>We also developed a sample Python Flask application to illustrate deployment: <a href=\"http://predictcustomerchurn.mybluemix.net/\">http://predictcustomerchurn.mybluemix.net/</a>. This application implements the REST client call to the model.</p>\n</div>"}]}},{"text":"%md\n### Step 1: Load Data","user":"sidneyp","dateUpdated":"2018-03-01T00:07:47+0000","config":{"lineNumbers":false,"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550153_-1492132321","id":"20171216-051104_1077485435","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:07:47+0000","dateFinished":"2018-03-01T00:07:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4351","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 1: Load Data</h3>\n</div>"}]}},{"title":"","text":"%spark.pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\n# Add asset from file system\r\nspark = SparkSession.builder.getOrCreate()\r\ncustomer = spark.read.csv('./datasets/customer.csv', header='true', inferSchema = 'true')\r\ncustomer.show(5)\r\n","user":"sidneyp","dateUpdated":"2018-03-01T00:02:26+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550153_-1492132321","id":"20171213-233843_297327902","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:02:26+0000","dateFinished":"2018-03-01T00:03:16+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4352"},{"title":"","text":"%spark.pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\n# Add asset from file system\r\nspark = SparkSession.builder.getOrCreate()\r\nchurn = spark.read.csv('./datasets/churn.csv', header='true', inferSchema = 'true')\r\nchurn.show(5)\r\n","user":"sidneyp","dateUpdated":"2018-03-01T00:03:38+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550154_-1490978074","id":"20171213-234011_1877573928","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:03:38+0000","dateFinished":"2018-03-01T00:03:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4353"},{"text":"%md\n### Step 2: Merge Files","user":"sidneyp","dateUpdated":"2018-03-01T00:08:01+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550154_-1490978074","id":"20171216-051502_1142500346","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:01+0000","dateFinished":"2018-03-01T00:08:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4354","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 2: Merge Files</h3>\n</div>"}]}},{"text":"%spark.pyspark\ndata=customer.join(churn,customer['ID']==churn['ID']).select(customer['*'],churn['CHURN'])","user":"sidneyp","dateUpdated":"2018-03-01T00:03:49+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550154_-1490978074","id":"20171213-234124_465356593","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:03:49+0000","dateFinished":"2018-03-01T00:03:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4355"},{"text":"%md\n### Step 3: Rename some columns","user":"sidneyp","dateUpdated":"2018-03-01T00:08:09+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550154_-1490978074","id":"20171216-051917_957183692","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:09+0000","dateFinished":"2018-03-01T00:08:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4356","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 3: Rename some columns</h3>\n</div>"}]}},{"text":"%spark.pyspark\ndata = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.show()","user":"sidneyp","dateUpdated":"2018-03-01T00:03:57+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550155_-1491362823","id":"20171213-234201_535748231","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:03:57+0000","dateFinished":"2018-03-01T00:03:59+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4357"},{"text":"%md\n\n### Step 4: Data Understanding\n\nZeppelin has a built-in query and visualization tool for querying Spark SQL tables. Use the %sql interpreter","user":"sidneyp","dateUpdated":"2018-03-01T00:08:13+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550155_-1491362823","id":"20171214-012032_443123673","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:13+0000","dateFinished":"2018-03-01T00:08:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4358","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 4: Data Understanding</h3>\n<p>Zeppelin has a built-in query and visualization tool for querying Spark SQL tables. Use the %sql interpreter</p>\n</div>"}]}},{"text":"%spark.pyspark\ndata.createOrReplaceTempView(\"data\")","user":"sidneyp","dateUpdated":"2018-03-01T00:04:03+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550155_-1491362823","id":"20171217-224554_1687836948","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:03+0000","dateFinished":"2018-03-01T00:04:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4359"},{"text":"%sql\nselect CHURN, LocalBilltype, avg(EstIncome) from data GROUP BY CHURN, LocalBillType ","user":"sidneyp","dateUpdated":"2018-03-01T00:04:08+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"multiBarChart","height":302,"optionOpen":false,"setting":{"multiBarChart":{"stacked":true}},"commonSetting":{},"keys":[{"name":"CHURN","index":0,"aggr":"sum"}],"groups":[{"name":"LocalBilltype","index":1,"aggr":"sum"}],"values":[{"name":"avg(EstIncome)","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550155_-1491362823","id":"20171217-224927_406860466","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:08+0000","dateFinished":"2018-03-01T00:04:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4360"},{"text":"%sql\nselect Paymethod, count(1) value from data Group By Paymethod\n","user":"sidneyp","dateUpdated":"2018-03-01T00:04:11+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550156_-1493286567","id":"20171228-203915_74519789","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:11+0000","dateFinished":"2018-03-01T00:04:16+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4361"},{"text":"%sql\nselect Age, count(1) value from data where Age<${maxAge=50} group by Age order by Age","user":"sidneyp","dateUpdated":"2018-03-01T00:04:20+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"Age","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"value","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{"maxAge":"50"},"forms":{"maxAge":{"name":"maxAge","defaultValue":"50","hidden":false,"$$hashKey":"object:5469"}}},"apps":[],"jobName":"paragraph_1514929550156_-1493286567","id":"20171228-193329_353627505","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:20+0000","dateFinished":"2018-03-01T00:04:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4362"},{"text":"%sql\nselect Age, count(1) value from data where Paymethod=\"${Paymethod=Auto,Auto|CC|CH}\" group by Age order by Age","user":"sidneyp","dateUpdated":"2018-03-01T00:04:24+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sql","runOnSelectionChange":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{"Paymethod":"CC"},"forms":{"Paymethod":{"name":"Paymethod","defaultValue":"Auto","options":[{"value":"Auto","$$hashKey":"object:5516"},{"value":"CC","$$hashKey":"object:5517"},{"value":"CH","$$hashKey":"object:5518"}],"hidden":false,"$$hashKey":"object:5508"}}},"apps":[],"jobName":"paragraph_1514929550156_-1493286567","id":"20171228-195511_799205977","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:24+0000","dateFinished":"2018-03-01T00:04:26+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4363"},{"title":"Interact with the graph by dragging \"Paymethod\" into the \"group\" box","text":"%sql\nselect LongDistance, Usage, Paymethod from data\n","user":"sidneyp","dateUpdated":"2018-03-01T00:04:30+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"setting":{"scatterChart":{"xAxis":{"name":"LongDistance","index":0,"aggr":"sum"},"yAxis":{"name":"Usage","index":1,"aggr":"sum"}}}},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550156_-1493286567","id":"20171228-204103_1157179934","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:04:30+0000","dateFinished":"2018-03-01T00:04:30+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4364"},{"text":"%md\n### Step 5: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models.  Additional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html","user":"sidneyp","dateUpdated":"2018-03-01T00:08:38+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550158_-1492517070","id":"20171216-052742_1853912173","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:38+0000","dateFinished":"2018-03-01T00:08:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4365","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 5: Build the Spark pipeline and the Random Forest model</h3>\n<p>&ldquo;Pipeline&rdquo; is an API in SparkML that&rsquo;s used for building models. Additional information on SparkML: <a href=\"https://spark.apache.org/docs/2.0.2/ml-guide.html\">https://spark.apache.org/docs/2.0.2/ml-guide.html</a></p>\n</div>"}]}},{"text":"%spark.pyspark\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Prepare string variables so that they can be used by the decision tree algorithm\n# StringIndexer encodes a string column of labels to a column of label indices\nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)\n\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")","user":"sidneyp","dateUpdated":"2018-03-01T00:05:28+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550158_-1492517070","id":"20171214-012318_976903634","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:05:28+0000","dateFinished":"2018-03-01T00:05:29+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4366"},{"text":"%spark.pyspark\n# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer,assembler, rf, labelConverter])\n","user":"sidneyp","dateUpdated":"2018-03-01T00:05:35+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550158_-1492517070","id":"20171214-013012_1971241348","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:05:35+0000","dateFinished":"2018-03-01T00:05:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4367"},{"text":"%spark.pyspark\n# Split data into train and test datasets\ntrain, test = data.randomSplit([0.8,0.2], seed=6)\ntrain.cache()\ntest.cache()\n","user":"sidneyp","dateUpdated":"2018-03-01T00:05:38+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550158_-1492517070","id":"20171214-013037_1711395197","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:05:38+0000","dateFinished":"2018-03-01T00:05:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4368"},{"text":"%spark.pyspark\n# Build models\nmodel = pipeline.fit(train)","user":"sidneyp","dateUpdated":"2018-03-01T00:05:41+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550158_-1492517070","id":"20171214-013118_114761329","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:05:41+0000","dateFinished":"2018-03-01T00:05:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4369"},{"text":"%md\n### Step 6: Score the test data set","user":"sidneyp","dateUpdated":"2018-03-01T00:08:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550159_-1492901818","id":"20171216-053132_440220050","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:50+0000","dateFinished":"2018-03-01T00:08:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4370","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 6: Score the test data set</h3>\n</div>"}]}},{"text":"%spark.pyspark\nresults = model.transform(test)\nresults=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\nresults.show(10,False)\n","user":"sidneyp","dateUpdated":"2018-03-01T00:05:53+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550159_-1492901818","id":"20171214-013225_491778721","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:05:54+0000","dateFinished":"2018-03-01T00:05:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4371"},{"text":"%md\n### Step 7: Model Evaluation\n","user":"sidneyp","dateUpdated":"2018-03-01T00:08:57+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550161_-1482898347","id":"20171216-053248_2147008432","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:08:57+0000","dateFinished":"2018-03-01T00:08:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4372","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 7: Model Evaluation</h3>\n</div>"}]}},{"text":"%spark.pyspark\nprint 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))\n","user":"sidneyp","dateUpdated":"2018-03-01T00:06:01+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550161_-1482898347","id":"20171214-013252_1580569700","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:06:01+0000","dateFinished":"2018-03-01T00:06:02+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4373"},{"text":"%spark.pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))\n","user":"sidneyp","dateUpdated":"2018-03-01T00:06:05+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550161_-1482898347","id":"20171214-013324_1966008016","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:06:05+0000","dateFinished":"2018-03-01T00:06:06+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4374"},{"text":"%md\n### Step 8: Save model in the repository\n**Note: If you wish, change model name**","user":"sidneyp","dateUpdated":"2018-03-01T00:09:04+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550161_-1482898347","id":"20171229-170420_1250438962","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:09:04+0000","dateFinished":"2018-03-01T00:09:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4375","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 8: Save model in the repository</h3>\n<p><strong>Note: If you wish, change model name</strong></p>\n</div>"}]}},{"text":"%spark.pyspark\nfrom dsx_ml.ml import save\nmodel_name=\"Telco Churn Zeppelin\"\nsave(model=model, name=model_name, test_data=test, algorithm_type=\"Classification\")\n","user":"sidneyp","dateUpdated":"2018-03-01T00:06:23+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550162_-1481744100","id":"20171229-170601_1089145165","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:06:23+0000","dateFinished":"2018-03-01T00:06:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4376"},{"text":"%md\n### Step 10: Test Saved Model with Test UI","user":"sidneyp","dateUpdated":"2018-03-01T00:09:09+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550162_-1481744100","id":"20171229-165825_157855583","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:09:09+0000","dateFinished":"2018-03-01T00:09:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4377","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Step 10: Test Saved Model with Test UI</h3>\n</div>"}]}},{"text":"%md\n1. Save the notebook and switch to the **Models** tab of the project (**hint**: right click the project name link, DSX_Lo, at the top, and open with another tab in your browser). \n2. Under **Models**, find and click into your saved model. \n4. Click the **Test** link to test the model. You can use the following data for testing: <br/>\n`ID=99, Gender=M, Status=S, Children=0, Est Income=60000, Car Owner=Y, Age=34, LongDistance=68, International=50, Local=100, Dropped=0, Paymethod=CC, LocalBilltype=Budget, LongDistanceBilltype=Intnl_discount, Usage=334, RatePlan=3`\n\nThe results of the test is displayed as follows:<br/>\n<img style=\"float: left;\" src=\"https://github.com/yfphoon/dsx_local/blob/master/images/Test_Model.png?raw=true\" alt=\"Test API\" width=900 />","user":"sidneyp","dateUpdated":"2018-03-01T00:09:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550162_-1481744100","id":"20171229-170650_1928076166","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:09:14+0000","dateFinished":"2018-03-01T00:09:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4378","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ol>\n  <li>Save the notebook and switch to the <strong>Models</strong> tab of the project (**hint**: right click the project name link, DSX_Lo, at the top, and open with another tab in your browser).</li>\n  <li>Under <strong>Models</strong>, find and click into your saved model.</li>\n  <li>Click the <strong>Test</strong> link to test the model. You can use the following data for testing: <br/><br/><code>ID=99, Gender=M, Status=S, Children=0, Est Income=60000, Car Owner=Y, Age=34, LongDistance=68, International=50, Local=100, Dropped=0, Paymethod=CC, LocalBilltype=Budget, LongDistanceBilltype=Intnl_discount, Usage=334, RatePlan=3</code></li>\n</ol>\n<p>The results of the test is displayed as follows:<br/><br/><img style=\"float: left;\" src=\"https://github.com/yfphoon/dsx_local/blob/master/images/Test_Model.png?raw=true\" alt=\"Test API\" width=900 /></p>\n</div>"}]}},{"text":"%md\n**Authors**:\n* Sidney Phoon\n* Rich Tarro\n* Elena Lowery\n\nLast Updated: 28th Dec, 2017","user":"sidneyp","dateUpdated":"2018-03-01T00:09:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550162_-1481744100","id":"20171228-001207_2056985975","dateCreated":"2018-01-02T21:45:50+0000","dateStarted":"2018-03-01T00:09:23+0000","dateFinished":"2018-03-01T00:09:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4379","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Authors</strong>:<br/>* Sidney Phoon<br/>* Rich Tarro<br/>* Elena Lowery</p>\n<p>Last Updated: 28th Dec, 2017</p>\n</div>"}]}},{"text":"%md\n","dateUpdated":"2018-01-02T21:45:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514929550162_-1481744100","id":"20171228-210713_1736472837","dateCreated":"2018-01-02T21:45:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4380"}],"name":"TelcoChurn_Zeppelin","id":"2D3TXZS9J","angularObjects":{"2CYSFF4KX:shared_process":[],"2CZ7M5Z1U:shared_process":[],"2CZN4E536:shared_process":[],"2CXMXB6B9:shared_process":[],"2CX1UN2AM:shared_process":[],"2CVYP86A3:shared_process":[],"2CZPMT4AR:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}